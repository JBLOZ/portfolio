{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed824d4c",
   "metadata": {},
   "source": [
    "# Análisis del Dataset Zoo - Clasificación Multiclase\n",
    "\n",
    "Este notebook implementa y evalúa 6 algoritmos de clasificación en el dataset Zoo de UCI:\n",
    "1. Naive Bayes Gaussiano\n",
    "2. MLE Multivariante (Full Bayesian Gaussian)\n",
    "3. Histogram Bayes\n",
    "4. Parzen Windows\n",
    "5. k-NN Density Bayes\n",
    "6. k-NN Rule\n",
    "\n",
    "Dataset: 17 atributos (15 binarios + 1 numérico + 1 clase), 7 clases de animales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ef993",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5f49f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Silenciar warnings\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '4'\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='joblib')\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7b966",
   "metadata": {},
   "source": [
    "## 2. Carga y análisis exploratorio del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be517131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANÁLISIS DEL DATASET ZOO (Multiclass Classification)\n",
      "================================================================================\n",
      "\n",
      "Información del dataset:\n",
      "Forma: (287, 16) (instancias x features)\n",
      "Clases: 7 (multiclass: ['mammal', 'bird', 'reptile', 'fish', 'amphibian', 'invertebrate', 'insect'])\n",
      "\n",
      "Distribución de clases:\n",
      "Clase 1 (mammal): 41 muestras (14.3%)\n",
      "Clase 2 (bird): 41 muestras (14.3%)\n",
      "Clase 3 (reptile): 41 muestras (14.3%)\n",
      "Clase 4 (fish): 41 muestras (14.3%)\n",
      "Clase 5 (amphibian): 41 muestras (14.3%)\n",
      "Clase 6 (invertebrate): 41 muestras (14.3%)\n",
      "Clase 7 (insect): 41 muestras (14.3%)\n",
      "\n",
      "Primeras 5 filas del dataset:\n",
      "     animal  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0  aardvark          1          0          0          1          0          0   \n",
      "1  antelope          1          0          0          1          0          0   \n",
      "2      bear          1          0          0          1          0          0   \n",
      "3      boar          1          0          0          1          0          0   \n",
      "4   buffalo          1          0          0          1          0          0   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  feature_11  feature_12  \\\n",
      "0          1          1          1           1           0           0   \n",
      "1          0          1          1           1           0           0   \n",
      "2          1          1          1           1           0           0   \n",
      "3          1          1          1           1           0           0   \n",
      "4          0          1          1           1           0           0   \n",
      "\n",
      "   feature_13  feature_14  feature_15  feature_16  class  \n",
      "0           4           0           0           1      1  \n",
      "1           4           1           0           1      1  \n",
      "2           4           0           0           1      1  \n",
      "3           4           1           0           1      1  \n",
      "4           4           1           0           1      1  \n"
     ]
    }
   ],
   "source": [
    "# Clases del Zoo dataset (1-7 -> labels para report)\n",
    "class_names = ['mammal', 'bird', 'reptile', 'fish', 'amphibian', 'invertebrate', 'insect']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DEL DATASET ZOO (Multiclass Classification)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cargar datos: zoo.data (col 0: animal name (ignorar), col 1-17: features, col 18: class 1-7)\n",
    "df = pd.read_csv('./data_zoo/zoo_balanced.csv')\n",
    "# df = pd.read_csv('./data_zoo/zoo.csv')\n",
    "df.columns = ['animal'] + [f'feature_{i}' for i in range(1, 17)] + ['class']\n",
    "X = df.iloc[:, 1:-1]  # Features 1-17\n",
    "y = df.iloc[:, -1].values - 1  # Clase 0-6 para sklearn\n",
    "\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(f\"Forma: {X.shape} (instancias x features)\")\n",
    "print(f\"Clases: {len(np.unique(y))} (multiclass: {class_names})\")\n",
    "print(\"\\nDistribución de clases:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for i, (cls, count) in enumerate(zip(class_names, counts)):\n",
    "    print(f\"Clase {i+1} ({cls}): {count} muestras ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95066c",
   "metadata": {},
   "source": [
    "## 3. División del dataset (Train-Test) y configuración de validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0eaad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de muestras: 287\n",
      "Train: 229 (79.8%)\n",
      "Test: 58 (20.2%)\n",
      "\n",
      "Distribución en train:\n",
      "0    32\n",
      "1    33\n",
      "2    33\n",
      "3    32\n",
      "4    33\n",
      "5    33\n",
      "6    33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución en test:\n",
      "0    9\n",
      "1    8\n",
      "2    8\n",
      "3    9\n",
      "4    8\n",
      "5    8\n",
      "6    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Configuración de validación cruzada: 5-fold estratificada\n"
     ]
    }
   ],
   "source": [
    "# División: 80% train - 20% test, estratificada para multiclass\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=41, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Total de muestras: {len(X)}\")\n",
    "print(f\"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDistribución en train:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(\"\\nDistribución en test:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())\n",
    "\n",
    "# CV estratificado (5 folds) para train\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=41)\n",
    "print(\"\\n✓ Configuración de validación cruzada: 5-fold estratificada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb11529",
   "metadata": {},
   "source": [
    "## 4. Función auxiliar para evaluación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0df8d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Función de evaluación definida\n"
     ]
    }
   ],
   "source": [
    "# Función helper para evaluar modelo (pred en test + report)\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1_mac = f1_score(y_test, preds, average='macro')\n",
    "    print(f\"\\n--- Resultados en Test ({model_name}) ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1-macro: {f1_mac:.4f}\")\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(classification_report(y_test, preds, target_names=class_names))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(cm)\n",
    "    return acc, f1_mac, cm\n",
    "\n",
    "print(\"✓ Función de evaluación definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55e565",
   "metadata": {},
   "source": [
    "## 5. Modelo 1: Naive Bayes Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9a35749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. NAIVE BAYES GAUSSIANO\n",
      "================================================================================\n",
      "\n",
      "--- Resultados en Test (Naive Bayes) ---\n",
      "Accuracy: 0.8966\n",
      "F1-macro: 0.8885\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       1.00      1.00      1.00         9\n",
      "        bird       1.00      1.00      1.00         8\n",
      "     reptile       1.00      0.50      0.67         8\n",
      "        fish       1.00      1.00      1.00         9\n",
      "   amphibian       0.64      0.88      0.74         8\n",
      "invertebrate       0.89      1.00      0.94         8\n",
      "      insect       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.90        58\n",
      "   macro avg       0.91      0.89      0.89        58\n",
      "weighted avg       0.92      0.90      0.89        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 0 0]\n",
      " [0 0 4 0 4 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 0 0 7 0 1]\n",
      " [0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 1 7]]\n",
      "\n",
      "CV F1-macro (mean ± std): 0.8782 ± 0.0354\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"1. NAIVE BAYES GAUSSIANO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_acc, nb_f1, nb_cm = evaluate_model(nb, X_test, y_test, \"Naive Bayes\")\n",
    "\n",
    "# CV score para NB (sin hypers)\n",
    "nb_cv_scores = cross_val_score(nb, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "print(f\"\\nCV F1-macro (mean ± std): {nb_cv_scores.mean():.4f} ± {nb_cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f99bde",
   "metadata": {},
   "source": [
    "## 6. Modelo 2: MLE Multivariante (Full Bayesian Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb3ab5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "2. MLE MULTIVARIANTE (Full Bayesian Gaussian)\n",
      "================================================================================\n",
      "✓ Clase FullGaussianBayes definida\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"2. MLE MULTIVARIANTE (Full Bayesian Gaussian)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class FullGaussianBayes:\n",
    "    def __init__(self):\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.covs = None\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.priors = np.bincount(y) / len(y)\n",
    "        self.means = np.array([X[y == c].mean(axis=0) for c in self.classes])\n",
    "        self.covs = np.array([np.cov(X[y == c].T) + 1e-6 * np.eye(X.shape[1]) for c in self.classes])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        ll = np.zeros((n_samples, len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            ll[:, i] = multivariate_normal(mean=self.means[i], cov=self.covs[i]).logpdf(X)\n",
    "        posteriors = np.exp(ll) * self.priors\n",
    "        posteriors /= posteriors.sum(axis=1, keepdims=True)\n",
    "        return np.argmax(posteriors, axis=1)\n",
    "\n",
    "print(\"✓ Clase FullGaussianBayes definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59966e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados en Test (MLE Full) ---\n",
      "Accuracy: 0.9310\n",
      "F1-macro: 0.9281\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       0.90      1.00      0.95         9\n",
      "        bird       1.00      1.00      1.00         8\n",
      "     reptile       1.00      0.88      0.93         8\n",
      "        fish       1.00      1.00      1.00         9\n",
      "   amphibian       0.88      0.88      0.88         8\n",
      "invertebrate       0.89      1.00      0.94         8\n",
      "      insect       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.93        58\n",
      "   macro avg       0.93      0.93      0.93        58\n",
      "weighted avg       0.93      0.93      0.93        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 0 0]\n",
      " [0 0 7 0 1 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 0 0 7 0 1]\n",
      " [0 0 0 0 0 8 0]\n",
      " [1 0 0 0 0 1 6]]\n",
      "\n",
      "CV F1-macro (mean ± std): 0.8889 ± 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\AppData\\Local\\Temp\\ipykernel_8164\\2355046959.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  posteriors /= posteriors.sum(axis=1, keepdims=True)\n",
      "C:\\Users\\jordi\\AppData\\Local\\Temp\\ipykernel_8164\\2355046959.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  posteriors /= posteriors.sum(axis=1, keepdims=True)\n",
      "C:\\Users\\jordi\\AppData\\Local\\Temp\\ipykernel_8164\\2355046959.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  posteriors /= posteriors.sum(axis=1, keepdims=True)\n",
      "C:\\Users\\jordi\\AppData\\Local\\Temp\\ipykernel_8164\\2355046959.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  posteriors /= posteriors.sum(axis=1, keepdims=True)\n",
      "C:\\Users\\jordi\\AppData\\Local\\Temp\\ipykernel_8164\\2355046959.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  posteriors /= posteriors.sum(axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "mle = FullGaussianBayes()\n",
    "mle.fit(X_train.values, y_train)\n",
    "mle_acc, mle_f1, mle_cm = evaluate_model(mle, X_test.values, y_test, \"MLE Full\")\n",
    "\n",
    "# CV para MLE (custom)\n",
    "def cv_full_bayes(X_train, y_train, cv):\n",
    "    scores = []\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model = FullGaussianBayes()\n",
    "        model.fit(X_tr.values, y_tr)\n",
    "        preds = model.predict(X_val.values)\n",
    "        scores.append(f1_score(y_val, preds, average='macro'))\n",
    "    return np.array(scores)\n",
    "\n",
    "mle_cv_scores = cv_full_bayes(pd.DataFrame(X_train), y_train, skf)\n",
    "print(f\"\\nCV F1-macro (mean ± std): {mle_cv_scores.mean():.4f} ± {mle_cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e65472",
   "metadata": {},
   "source": [
    "## 7. Modelo 3: Histogram Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "87ec2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "3. DENSIDAD NO PARAMÉTRICA - HISTOGRAMA\n",
      "================================================================================\n",
      "✓ Clase HistogramBayes definida\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"3. DENSIDAD NO PARAMÉTRICA - HISTOGRAMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class HistogramBayes:\n",
    "    def __init__(self, bins=2):\n",
    "        self.bins = bins\n",
    "        self.priors = None\n",
    "        self.hist_per_class = None\n",
    "        self.edges = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.priors = np.bincount(y) / len(y)\n",
    "        self.hist_per_class = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            hists = []\n",
    "            edges_list = []\n",
    "            for feat in range(X.shape[1]):\n",
    "                hist, edges = np.histogram(X_c.iloc[:, feat], bins=self.bins, density=True)\n",
    "                hists.append(hist)\n",
    "                edges_list.append(edges)\n",
    "            self.hist_per_class[c] = (np.array(hists), edges_list)\n",
    "        self.edges = edges_list[0] if edges_list else None\n",
    "        return self\n",
    "    \n",
    "    def _density_hist(self, x, c):\n",
    "        hists, edges = self.hist_per_class[c]\n",
    "        dens = 1.0\n",
    "        for i, feat_val in enumerate(x):\n",
    "            bin_idx = np.digitize(feat_val, edges[i]) - 1\n",
    "            if 0 <= bin_idx < len(hists[i]):\n",
    "                dens *= hists[i][bin_idx]\n",
    "            else:\n",
    "                dens *= 0\n",
    "        return dens\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = len(X)\n",
    "        preds = np.zeros(n_samples, dtype=int)\n",
    "        for i in range(n_samples):\n",
    "            posteriors = []\n",
    "            for c in self.classes:\n",
    "                dens = self._density_hist(X.iloc[i], c)\n",
    "                post = self.priors[c] * dens\n",
    "                posteriors.append(post)\n",
    "            preds[i] = self.classes[np.argmax(posteriors)]\n",
    "        return preds\n",
    "\n",
    "print(\"✓ Clase HistogramBayes definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9dd7251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados en Test (Histogram Bayes) ---\n",
      "Accuracy: 0.1724\n",
      "F1-macro: 0.0707\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       0.16      1.00      0.27         9\n",
      "        bird       0.00      0.00      0.00         8\n",
      "     reptile       0.00      0.00      0.00         8\n",
      "        fish       0.00      0.00      0.00         9\n",
      "   amphibian       0.00      0.00      0.00         8\n",
      "invertebrate       1.00      0.12      0.22         8\n",
      "      insect       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.17        58\n",
      "   macro avg       0.17      0.16      0.07        58\n",
      "weighted avg       0.16      0.17      0.07        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [8 0 0 0 0 0 0]\n",
      " [8 0 0 0 0 0 0]\n",
      " [9 0 0 0 0 0 0]\n",
      " [8 0 0 0 0 0 0]\n",
      " [7 0 0 0 0 1 0]\n",
      " [8 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV F1-macro (mean ± std): 0.0599 ± 0.0343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_bayes = HistogramBayes(bins=2)\n",
    "hist_bayes.fit(pd.DataFrame(X_train), y_train)\n",
    "hist_acc, hist_f1, hist_cm = evaluate_model(hist_bayes, pd.DataFrame(X_test), y_test, \"Histogram Bayes\")\n",
    "\n",
    "# CV para Histogram (custom)\n",
    "def cv_hist_bayes(X_train, y_train, cv):\n",
    "    scores = []\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model = HistogramBayes()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(f1_score(y_val, preds, average='macro'))\n",
    "    return np.array(scores)\n",
    "\n",
    "hist_cv_scores = cv_hist_bayes(pd.DataFrame(X_train), y_train, skf)\n",
    "print(f\"\\nCV F1-macro (mean ± std): {hist_cv_scores.mean():.4f} ± {hist_cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e6554",
   "metadata": {},
   "source": [
    "## 8. Modelo 4: Parzen Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fe81d",
   "metadata": {},
   "source": [
    "**Nota sobre CV:** La validación cruzada 5-fold divide el conjunto de entrenamiento en 5 partes. Para cada configuración de hiperparámetro, se entrena y evalúa 5 veces (una por fold), obteniendo 5 scores de F1-macro. El **mean** indica el rendimiento promedio y el **std** indica la estabilidad del modelo (std bajo = rendimiento consistente entre folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ab12c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. DENSIDAD NO PARAMÉTRICA - PARZEN WINDOWS\n",
      "================================================================================\n",
      "✓ Clase ParzenBayes definida\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"4. DENSIDAD NO PARAMÉTRICA - PARZEN WINDOWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class ParzenBayes:\n",
    "    def __init__(self, bandwidth=0.5):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.priors = None\n",
    "        self.kdes = None\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.priors = np.bincount(y) / len(y)\n",
    "        self.kdes = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c].values.reshape(-1, X.shape[1])\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=self.bandwidth).fit(X_c)\n",
    "            self.kdes[c] = kde\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_val = X.values.reshape(-1, X.shape[1])\n",
    "        n_samples = len(X_val)\n",
    "        ll = np.zeros((n_samples, len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            ll[:, i] = np.exp(self.kdes[c].score_samples(X_val))\n",
    "        posteriors = ll * self.priors\n",
    "        posteriors /= posteriors.sum(axis=1, keepdims=True) + 1e-10\n",
    "        return np.argmax(posteriors, axis=1)\n",
    "\n",
    "print(\"✓ Clase ParzenBayes definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a72e533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Búsqueda de hiperparámetros (en train) ---\n",
      "h=0.05: F1-macro CV = 0.9302\n",
      "h=0.1: F1-macro CV = 0.9350\n",
      "h=0.5: F1-macro CV = 0.9388\n",
      "h=1.0: F1-macro CV = 0.8729\n",
      "h=1.5: F1-macro CV = 0.7364\n",
      "h=2.0: F1-macro CV = 0.5948\n",
      "\n",
      "✓ Mejor bandwidth h: 0.5\n",
      "✓ CV F1-macro (mean ± std): 0.9388 ± 0.0258\n"
     ]
    }
   ],
   "source": [
    "# GridSearch para bandwidth (h)\n",
    "print(\"\\n--- Búsqueda de hiperparámetros (en train) ---\")\n",
    "params_parzen = {'bandwidth': [0.05,0.1, 0.5, 1.0, 1.5, 2.0]}\n",
    "\n",
    "best_h = None\n",
    "best_cv_score = -np.inf\n",
    "best_cv_scores = None  # Guardar los 5 scores del mejor\n",
    "\n",
    "for h in params_parzen['bandwidth']:\n",
    "    model_temp = ParzenBayes(bandwidth=h)\n",
    "    cv_scores_temp = []\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model_temp.fit(X_tr, y_tr)\n",
    "        preds_temp = model_temp.predict(X_val)\n",
    "        cv_scores_temp.append(f1_score(y_val, preds_temp, average='macro'))\n",
    "    mean_score = np.mean(cv_scores_temp)\n",
    "    print(f\"h={h}: F1-macro CV = {mean_score:.4f}\")\n",
    "    if mean_score > best_cv_score:\n",
    "        best_cv_score = mean_score\n",
    "        best_h = h\n",
    "        best_cv_scores = cv_scores_temp  # Guardar scores del mejor\n",
    "\n",
    "print(f\"\\n✓ Mejor bandwidth h: {best_h}\")\n",
    "print(f\"✓ CV F1-macro (mean ± std): {np.mean(best_cv_scores):.4f} ± {np.std(best_cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b29e202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados en Test (Parzen Bayes) ---\n",
      "Accuracy: 0.9483\n",
      "F1-macro: 0.9457\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       1.00      1.00      1.00         9\n",
      "        bird       1.00      1.00      1.00         8\n",
      "     reptile       1.00      0.88      0.93         8\n",
      "        fish       0.90      1.00      0.95         9\n",
      "   amphibian       0.89      1.00      0.94         8\n",
      "invertebrate       0.89      1.00      0.94         8\n",
      "      insect       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.95        58\n",
      "   macro avg       0.95      0.95      0.95        58\n",
      "weighted avg       0.95      0.95      0.95        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 0 0]\n",
      " [0 0 7 0 1 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 0 0 8 0 0]\n",
      " [0 0 0 0 0 8 0]\n",
      " [0 0 0 1 0 1 6]]\n"
     ]
    }
   ],
   "source": [
    "# Entrenar con best h y evaluar\n",
    "parzen_bayes = ParzenBayes(bandwidth=best_h)\n",
    "parzen_bayes.fit(pd.DataFrame(X_train), y_train)\n",
    "parzen_acc, parzen_f1, parzen_cm = evaluate_model(parzen_bayes, pd.DataFrame(X_test), y_test, \"Parzen Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17b910",
   "metadata": {},
   "source": [
    "## 9. Modelo 5: k-NN Density Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "901ccaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "5. DENSIDAD NO PARAMÉTRICA - k-NN ESTIMATOR\n",
      "================================================================================\n",
      "✓ Clase KNNDensityBayes definida\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"5. DENSIDAD NO PARAMÉTRICA - k-NN ESTIMATOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class KNNDensityBayes:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.priors = None\n",
    "        self.kdes = None\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.priors = np.bincount(y) / len(y)\n",
    "        self.kdes = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c].values.reshape(-1, X.shape[1])\n",
    "            bandwidth = 1.0 / np.sqrt(self.k / len(X_c)) if len(X_c) > 0 else 0.5\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth, algorithm='kd_tree').fit(X_c)\n",
    "            self.kdes[c] = kde\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_val = X.values.reshape(-1, X.shape[1])\n",
    "        n_samples = len(X_val)\n",
    "        ll = np.zeros((n_samples, len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            ll[:, i] = np.exp(self.kdes[c].score_samples(X_val))\n",
    "        posteriors = ll * self.priors\n",
    "        posteriors /= posteriors.sum(axis=1, keepdims=True) + 1e-10\n",
    "        return np.argmax(posteriors, axis=1)\n",
    "\n",
    "print(\"✓ Clase KNNDensityBayes definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9b6c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Búsqueda de hiperparámetros (en train) ---\n",
      "k=3: F1-macro CV = 0.5710\n",
      "k=5: F1-macro CV = 0.5860\n",
      "k=7: F1-macro CV = 0.6088\n",
      "k=9: F1-macro CV = 0.6309\n",
      "k=11: F1-macro CV = 0.6501\n",
      "\n",
      "✓ Mejor k: 11\n",
      "✓ CV F1-macro (mean ± std): 0.6501 ± 0.0670\n",
      "k=7: F1-macro CV = 0.6088\n",
      "k=9: F1-macro CV = 0.6309\n",
      "k=11: F1-macro CV = 0.6501\n",
      "\n",
      "✓ Mejor k: 11\n",
      "✓ CV F1-macro (mean ± std): 0.6501 ± 0.0670\n"
     ]
    }
   ],
   "source": [
    "# GridSearch para k\n",
    "print(\"\\n--- Búsqueda de hiperparámetros (en train) ---\")\n",
    "params_knn_density = [3, 5, 7, 9, 11]\n",
    "best_k_density = None\n",
    "best_cv_score_density = -np.inf\n",
    "best_cv_scores_density = None  # Guardar los 5 scores del mejor\n",
    "\n",
    "for k in params_knn_density:\n",
    "    model_temp = KNNDensityBayes(k=k)\n",
    "    cv_scores_temp = []\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model_temp.fit(X_tr, y_tr)\n",
    "        preds_temp = model_temp.predict(X_val)\n",
    "        cv_scores_temp.append(f1_score(y_val, preds_temp, average='macro'))\n",
    "    mean_score = np.mean(cv_scores_temp)\n",
    "    print(f\"k={k}: F1-macro CV = {mean_score:.4f}\")\n",
    "    if mean_score > best_cv_score_density:\n",
    "        best_cv_score_density = mean_score\n",
    "        best_k_density = k\n",
    "        best_cv_scores_density = cv_scores_temp  # Guardar scores del mejor\n",
    "\n",
    "print(f\"\\n✓ Mejor k: {best_k_density}\")\n",
    "print(f\"✓ CV F1-macro (mean ± std): {np.mean(best_cv_scores_density):.4f} ± {np.std(best_cv_scores_density):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3fecc6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados en Test (k-NN Density Bayes) ---\n",
      "Accuracy: 0.7241\n",
      "F1-macro: 0.6182\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       1.00      1.00      1.00         9\n",
      "        bird       1.00      1.00      1.00         8\n",
      "     reptile       0.00      0.00      0.00         8\n",
      "        fish       0.47      1.00      0.64         9\n",
      "   amphibian       0.73      1.00      0.84         8\n",
      "invertebrate       0.73      1.00      0.84         8\n",
      "      insect       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.72        58\n",
      "   macro avg       0.56      0.71      0.62        58\n",
      "weighted avg       0.57      0.72      0.63        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 0 0]\n",
      " [0 0 0 5 3 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 0 0 8 0 0]\n",
      " [0 0 0 0 0 8 0]\n",
      " [0 0 0 5 0 3 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "knn_density_bayes = KNNDensityBayes(k=best_k_density)\n",
    "knn_density_bayes.fit(pd.DataFrame(X_train), y_train)\n",
    "knn_d_acc, knn_d_f1, knn_d_cm = evaluate_model(knn_density_bayes, pd.DataFrame(X_test), y_test, \"k-NN Density Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecd98e",
   "metadata": {},
   "source": [
    "## 10. Modelo 6: k-NN Rule (Directo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d29bdd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6. K-NEAREST NEIGHBORS RULE (Directo)\n",
      "================================================================================\n",
      "\n",
      "--- Búsqueda de hiperparámetros (en train) ---\n",
      "Buscando mejor k con CV 5-fold...\n",
      "\n",
      "✓ Mejor k: 3\n",
      "✓ CV F1-macro (mean ± std): 0.9359 ± 0.0338\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"6. K-NEAREST NEIGHBORS RULE (Directo)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Búsqueda de hiperparámetros (en train) ---\")\n",
    "print(\"Buscando mejor k con CV 5-fold...\")\n",
    "params_knn = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "grid_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(metric='euclidean'),\n",
    "    params_knn,\n",
    "    cv=skf,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    return_train_score=False\n",
    ")\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# Obtener std del mejor modelo (de los 5 folds)\n",
    "best_idx = grid_knn.best_index_\n",
    "best_std = grid_knn.cv_results_['std_test_score'][best_idx]\n",
    "\n",
    "print(f\"\\n✓ Mejor k: {grid_knn.best_params_['n_neighbors']}\")\n",
    "print(f\"✓ CV F1-macro (mean ± std): {grid_knn.best_score_:.4f} ± {best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f7e9ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados en Test (k-NN Rule) ---\n",
      "Accuracy: 0.9655\n",
      "F1-macro: 0.9637\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      mammal       1.00      1.00      1.00         9\n",
      "        bird       1.00      1.00      1.00         8\n",
      "     reptile       1.00      1.00      1.00         8\n",
      "        fish       0.90      1.00      0.95         9\n",
      "   amphibian       1.00      1.00      1.00         8\n",
      "invertebrate       0.89      1.00      0.94         8\n",
      "      insect       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.97        58\n",
      "   macro avg       0.97      0.96      0.96        58\n",
      "weighted avg       0.97      0.97      0.96        58\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "[[9 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 0 0]\n",
      " [0 0 8 0 0 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 0 0 8 0 0]\n",
      " [0 0 0 0 0 8 0]\n",
      " [0 0 0 1 0 1 6]]\n"
     ]
    }
   ],
   "source": [
    "best_knn = grid_knn.best_estimator_\n",
    "knn_acc, knn_f1, knn_cm = evaluate_model(best_knn, X_test, y_test, \"k-NN Rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505fd08",
   "metadata": {},
   "source": [
    "## 11. Comparación final: CV (Train) vs Test\n",
    "\n",
    "La siguiente sección muestra dos comparaciones:\n",
    "1. **Validación Cruzada en Train**: Rendimiento durante la selección de modelos/hiperparámetros (con std para medir estabilidad)\n",
    "2. **Evaluación en Test**: Rendimiento final en datos no vistos (sin std porque solo hay 1 evaluación)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741f256",
   "metadata": {},
   "source": [
    "## Resumen de Validación Cruzada (5-fold en Train)\n",
    "\n",
    "Todos los modelos fueron evaluados usando validación cruzada estratificada 5-fold **solo en el conjunto de entrenamiento**. El **mean** indica el rendimiento promedio y el **std** indica la variabilidad entre folds (menor std = mayor estabilidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "230c2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUMEN: VALIDACIÓN CRUZADA 5-FOLD (en Train)\n",
      "================================================================================\n",
      "\n",
      "Modelo                         F1-macro (mean)        std\n",
      "------------------------------------------------------------\n",
      "Naive Bayes                             0.8782     0.0354\n",
      "MLE Full                                0.8889     0.0595\n",
      "Histogram Bayes                         0.0599     0.0343\n",
      "Parzen (h=0.5)                          0.9388     0.0258\n",
      "k-NN Density (k=11)                     0.6501     0.0670\n",
      "k-NN Rule (k=3)                         0.9359     0.0338\n",
      "------------------------------------------------------------\n",
      "\n",
      "Interpretación: std bajo indica mayor estabilidad del modelo entre folds\n",
      "================================================================================\n",
      "\n",
      "RESUMEN: VALIDACIÓN CRUZADA 5-FOLD (en Train)\n",
      "================================================================================\n",
      "\n",
      "Modelo                         F1-macro (mean)        std\n",
      "------------------------------------------------------------\n",
      "Naive Bayes                             0.8782     0.0354\n",
      "MLE Full                                0.8889     0.0595\n",
      "Histogram Bayes                         0.0599     0.0343\n",
      "Parzen (h=0.5)                          0.9388     0.0258\n",
      "k-NN Density (k=11)                     0.6501     0.0670\n",
      "k-NN Rule (k=3)                         0.9359     0.0338\n",
      "------------------------------------------------------------\n",
      "\n",
      "Interpretación: std bajo indica mayor estabilidad del modelo entre folds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN: VALIDACIÓN CRUZADA 5-FOLD (en Train)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_results = {\n",
    "    'Naive Bayes': (nb_cv_scores.mean(), nb_cv_scores.std()),\n",
    "    'MLE Full': (mle_cv_scores.mean(), mle_cv_scores.std()),\n",
    "    'Histogram Bayes': (hist_cv_scores.mean(), hist_cv_scores.std()),\n",
    "    f'Parzen (h={best_h})': (np.mean(best_cv_scores), np.std(best_cv_scores)),\n",
    "    f'k-NN Density (k={best_k_density})': (np.mean(best_cv_scores_density), np.std(best_cv_scores_density)),\n",
    "    f'k-NN Rule (k={grid_knn.best_params_[\"n_neighbors\"]})': (grid_knn.best_score_, best_std)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Modelo':<30} {'F1-macro (mean)':>15} {'std':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for model, (mean, std) in cv_results.items():\n",
    "    print(f\"{model:<30} {mean:>15.4f} {std:>10.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nInterpretación: std bajo indica mayor estabilidad del modelo entre folds\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "59c5f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARACIÓN FINAL DE MODELOS (en Test)\n",
      "================================================================================\n",
      "\n",
      "Modelo                      Accuracy   F1-macro\n",
      "--------------------------------------------------\n",
      "Naive Bayes                   0.8966     0.8885\n",
      "MLE Full                      0.9310     0.9281\n",
      "Histogram Bayes               0.1724     0.0707\n",
      "Parzen Bayes                  0.9483     0.9457\n",
      "k-NN Density Bayes            0.7241     0.6182\n",
      "k-NN Rule (k=3)               0.9655     0.9637\n",
      "--------------------------------------------------\n",
      "\n",
      "✓ Mejor modelo (por F1-macro): k-NN Rule (k=3) (F1: 0.9637)\n",
      "\n",
      "================================================================================\n",
      "✓ Análisis completo finalizado\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARACIÓN FINAL DE MODELOS (en Test)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {\n",
    "    'Naive Bayes': (nb_acc, nb_f1),\n",
    "    'MLE Full': (mle_acc, mle_f1),\n",
    "    'Histogram Bayes': (hist_acc, hist_f1),\n",
    "    'Parzen Bayes': (parzen_acc, parzen_f1),\n",
    "    'k-NN Density Bayes': (knn_d_acc, knn_d_f1),\n",
    "    f'k-NN Rule (k={grid_knn.best_params_[\"n_neighbors\"]})': (knn_acc, knn_f1)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Modelo':<25} {'Accuracy':>10} {'F1-macro':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for model, (acc, f1) in results.items():\n",
    "    print(f\"{model:<25} {acc:>10.4f} {f1:>10.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Mejor modelo por F1-macro (prioridad para multiclass)\n",
    "best_model = max(results, key=lambda k: results[k][1])\n",
    "print(f\"\\n✓ Mejor modelo (por F1-macro): {best_model} (F1: {results[best_model][1]:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Análisis completo finalizado\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
