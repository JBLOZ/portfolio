% !TEX program = xelatex
% ¬°Recuerda compilar con XeLaTeX o LuaLaTeX!
\documentclass{article}

% --- Cargar nuestro fichero de estilo ---
% Se asume que paper_style.sty est√° disponible o se usan paquetes est√°ndar.
\usepackage{paper_style}

% --- PAQUETES PARA EL CONTENIDO DEL DOCUMENTO ---
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}


% --- Informaci√≥n del Paper ---
\title{Seminario 2: Bot de analisis del sentimiento de mercados financieros\\ }
\author{
	Alejandro Mart√≠nez Riquelme \\
	Jordi Blasco Lozano \\
	\small Interacci√≥n Persona M√°quina \\
	\small Universidad de Alicante
}
\date{\today}


% --- Comienzo del Documento ---
\begin{document}
	\sloppy % Relaja el ajuste de l√≠neas para reducir overfull hboxes en frases largas
	\maketitle

	\begin{abstract}
	\noindent 
	En un mundo financiero cada vez m√°s din√°mico y sobrecargado de informaci√≥n, tomar decisiones de inversi√≥n √°giles e informadas es un desaf√≠o constante. Este proyecto presenta el desarrollo de un asistente conversacional inteligente a trav√©s de un bot de Telegram, dise√±ado para realizar an√°lisis de sentimiento de mercado para cualquier acci√≥n o ETF bajo demanda. El sistema recopila autom√°ticamente noticias relevantes mediante la API de Perplexity, las analiza utilizando modelos de lenguaje avanzados con una arquitectura Actor-Cr√≠tico, y genera distribuciones de probabilidad para acciones de inversi√≥n (Compra/Mantener/Vender) junto con expectativas de tendencia y res√∫menes justificativos. La soluci√≥n integra m√∫ltiples tecnolog√≠as como n8n, Docker y Cloudflare Tunnel para crear una plataforma robusta y accesible que simplifica el acceso a inteligencia de mercado.
	\end{abstract}
	\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{image.png}
	\caption{Esquema n8n}
	\end{figure}
	\newpage
	\tableofcontents
	\newpage

	\section{Introducci√≥n al Proyecto: Bot de An√°lisis de Sentimiento Burs√°til}
	
	En un mundo financiero cada vez m√°s din√°mico y sobrecargado de informaci√≥n, tomar decisiones de inversi√≥n √°giles e informadas es un desaf√≠o constante. Los inversores, tanto novatos como experimentados, se enfrentan al reto de procesar grandes vol√∫menes de noticias, informes y opiniones para evaluar el sentimiento del mercado sobre un activo financiero. Este proceso no solo consume mucho tiempo, sino que tambi√©n requiere un an√°lisis complejo para extraer conclusiones pr√°cticas.
	
	Para abordar este problema, hemos desarrollado una soluci√≥n innovadora: un \textbf{asistente conversacional inteligente a trav√©s de un bot de Telegram}, dise√±ado para realizar an√°lisis de sentimiento de mercado para cualquier acci√≥n o ETF bajo demanda.
	
	Nuestro proyecto nace de la necesidad de simplificar y automatizar el acceso a inteligencia de mercado. El objetivo principal es proporcionar a cualquier usuario una herramienta accesible desde su m√≥vil que, con un simple comando, sea capaz de:
	\begin{enumerate}
		\item \textbf{Recopilar las noticias m√°s recientes} y relevantes sobre un activo financiero espec√≠fico.
		\item \textbf{Utilizar un Modelo de Lenguaje Avanzado (LLM)} para analizar esta informaci√≥n y determinar el sentimiento general del mercado.
		\item \textbf{Generar una distribuci√≥n de probabilidades} para las acciones de Compra, Mantener y Vender (en porcentajes que sumen 100\%), junto con una expectativa de tendencia (Alcista o Bajista) y un resumen justificativo basado en las noticias analizadas.
	\end{enumerate}
	
	Para lograrlo, hemos dise√±ado y construido una arquitectura robusta que integra m√∫ltiples tecnolog√≠as. Utilizando \textbf{n8n} como plataforma central de automatizaci√≥n para orquestar el flujo de trabajo, \textbf{Docker} para crear el entorno de ejecuci√≥n, y un t√∫nel de \textbf{Cloudflare} para exponer nuestro servicio local a Internet de forma segura y asi poder permitir la entrada de mensajes mediante nuestro bot de telegram.
	
	Este proyecto no solo resuelve un problema real, sino que tambi√©n sirve como una demostraci√≥n pr√°ctica de c√≥mo la combinaci√≥n de la automatizaci√≥n de flujos de trabajo, la inteligencia artificial conversacional y la infraestructura nos permite crear potentes aplicaciones con recursos limitados. A continuaci√≥n, explicaremos en detalle cada uno de los pasos que hemos seguido para dar vida a este bot, desde la configuraci√≥n del entorno hasta el dise√±o del flujo de inteligencia.
	
	\section{Configuraci√≥n del T√∫nel y Scripts de Inicio}
	
	Para que nuestro bot de Telegram pueda recibir y procesar mensajes en tiempo real, es fundamental exponer el servidor local de n8n a Internet, ya que Telegram requiere una URL p√∫blica y accesible para configurar el webhook del trigger. Sin esta exposici√≥n, el trigger de Telegram no podr√≠a funcionar, ya que n8n opera por defecto en localhost:5678, un puerto interno no visible desde fuera. Elegimos Cloudflare Tunnel (cloudflared) por su simplicidad y gratuidad para entornos de desarrollo, aunque sus URLs generadas no son est√°ticas y cambian en cada ejecuci√≥n, lo que impide hardcodearlas directamente en la configuraci√≥n de n8n.
	
	El desaf√≠o principal radica en esta variabilidad: si intent√°ramos definir una URL fija en el nodo de Telegram o en el docker-compose.yml, el webhook fallar√≠a al reiniciar el servicio. Por ello, optamos por un enfoque din√°mico basado en variables de entorno (.env), que permite inyectar la URL generada autom√°ticamente en el contenedor de n8n sin intervenci√≥n manual cada vez. Este m√©todo asegura que el trigger de Telegram apunte siempre a la URL correcta, usando variables como WEBHOOK\_URL, N8N\_HOST y TUNNEL\_URL para configurar el protocolo HTTPS y el host externo.
	
	\subsection{Paso 1: Instalaci√≥n y Preparaci√≥n de Cloudflare Tunnel}
	
	Primero, instalamos cloudflared en nuestro sistema operativo. Este herramienta crea un t√∫nel seguro desde nuestro puerto local (5678) hacia una URL temporal en el dominio trycloudflare.com, sin necesidad de cuenta ni configuraci√≥n compleja. El comando base para iniciarlo es \texttt{cloudflared tunnel --url http://localhost:5678}, que genera una salida como ``https://xxxx.trycloudflare.com'' y la registra en un log para su captura posterior. Este t√∫nel enruta el tr√°fico entrante de Telegram directamente a n8n, manteniendo la seguridad al no exponer puertos directamente en el firewall.
	
	\subsection{Paso 2: Creaci√≥n del Script de Inicio (start-n8n.sh)}
	
	Para automatizar todo el proceso, creamos el script \texttt{start-n8n.sh}, que sigue un orden espec√≠fico y secuencial para garantizar la integraci√≥n sin errores. El script comienza deteniendo cualquier instancia previa de \texttt{cloudflared} o Docker para evitar conflictos: \texttt{pkill cloudflared 2>/dev/null} y \texttt{docker-compose down 2>/dev/null}. Luego, lanza \texttt{cloudflared} en segundo plano con redirecci√≥n de salida a un archivo \texttt{tunnel.log}: \texttt{cloudflared tunnel --url http://localhost:5678 > tunnel.log 2>\&1 \&}, capturando el PID para control posterior.
	
	A continuaci√≥n, el script espera hasta 30 segundos para que se genere la URL, monitoreando el log con un bucle: \texttt{grep -oP 'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com' tunnel.log | head -1}. Una vez extra√≠da la URL (por ejemplo, \url{https://offline-argued-drops.trycloudflare.com}), la valida y la almacena en una variable TUNNEL\_URL. Si no se obtiene en el tiempo l√≠mite, el script falla con un error y mata el proceso de \texttt{cloudflared}. Este paso es cr√≠tico porque sin la URL din√°mica, el webhook de Telegram no se configurar√≠a correctamente en n8n.
	
	\subsection{Paso 3: Actualizaci√≥n de Variables de Entorno (.env)}
	
	Con la URL en mano, el script actualiza el archivo .env, que sirve como fuente de variables para docker-compose.yml. Genera o sobrescribe el .env con contenido como:
	\begin{itemize}
		\item WEBHOOK\_URL=\$\{TUNNEL\_URL\}
		\item N8N\_HOST=\$\{TUNNEL\_URL\} 
		\item TUNNEL\_URL=\$\{TUNNEL\_URL\}
	\end{itemize}
	
	Esto inyecta la URL en el entorno de n8n, permitiendo que el nodo de Telegram use autom√°ticamente WEBHOOK\_URL como endpoint p√∫blico. El script muestra el contenido del .env para verificaci√≥n antes de proceder. De esta forma, evitamos editar manualmente el .env cada reinicio, haciendo el despliegue reproducible con un solo comando: \texttt{./start-n8n.sh}.
	
	\subsection{Paso 4: Lanzamiento del Contenedor de n8n}
	
	Finalmente, con el .env actualizado, el script ejecuta \texttt{docker-compose up -d}, iniciando el contenedor de n8n con las variables cargadas. El \texttt{docker-compose.yml} define el servicio n8n con puertos expuestos en 5678, vol√∫menes para persistencia de datos (\texttt{/home/node/.n8n}), y variables como \texttt{N8N\_PROTOCOL=https} y \texttt{N8N\_EDITOR\_BASE\_URL=\$\{TUNNEL\_URL\}} para alinear la interfaz web y los webhooks. Una vez arriba, n8n configura internamente el webhook de Telegram apuntando a la URL del t√∫nel, completando el ciclo de exposici√≥n. Para detener todo, usamos un script complementario \texttt{stop-n8n.sh} que hace \texttt{down} en Docker y mata \texttt{cloudflared}, limpiando logs.
	
	Esta configuraci√≥n no solo resuelve la exposici√≥n din√°mica, sino que tambi√©n mantiene el proyecto portable y escalable, ideal para demostraciones como esta. En la siguiente secci√≥n, detallaremos la creaci√≥n del bot en Telegram y su integraci√≥n en el workflow de n8n.
	
	\section{Creaci√≥n del Bot de Telegram e Integraci√≥n del Trigger}
	
	Una vez configurado el t√∫nel para exponer n8n a Internet, el siguiente paso esencial es crear el bot en Telegram y conectarlo al workflow mediante el nodo de trigger de Telegram, que act√∫a como punto de entrada para recibir mensajes del usuario. Este trigger se basa en la API de Telegram Bot, que requiere un token de autenticaci√≥n para configurar un webhook que env√≠e actualizaciones de mensajes a nuestra URL p√∫blica (la generada por cloudflared). Sin esta integraci√≥n, el bot no podr√≠a interactuar con el flujo de n8n, ya que Telegram solo notifica a endpoints accesibles externamente.
	
	\subsection{Paso 1: Creaci√≥n del Bot con BotFather}
	
	Para iniciar, abrimos Telegram y buscamos el usuario oficial @BotFather, que es la herramienta de Telegram para gestionar bots. Enviamos el comando \texttt{/newbot} seguido del nombre del bot (por ejemplo, ``Stock Market Analytics Bot'') y un username √∫nico terminando en ``bot'' (como \texttt{@stock\_market\_analytics\_bot}), que debe ser p√∫blico y accesible para todos los usuarios. BotFather responde confirmando la creaci√≥n y proporcionando un \textbf{token de API} en formato \texttt{123456789:ABCDEF...}, que act√∫a como clave secreta para autenticar todas las peticiones del bot. Este token no debe compartirse p√∫blicamente, ya que otorga control total sobre el bot, incluyendo la capacidad de enviar y recibir mensajes.
	
	\subsection{Paso 2: Configuraci√≥n del Nodo de Trigger en n8n}
	
	Con el token en mano, accedemos a la interfaz de n8n (v√≠a la URL del t√∫nel) y creamos un nuevo workflow, arrastrando el nodo \textbf{Telegram Trigger} al canvas. En las credenciales del nodo, seleccionamos ``Create New'' para la autenticaci√≥n de Telegram, ingresando el token de API obtenido de BotFather y configurando el tipo como ``Bot Token Authentication''. El nodo tambi√©n requiere la URL del webhook, que se resuelve autom√°ticamente usando la variable de entorno WEBHOOK\_URL del .env (injected v√≠a docker-compose), apuntando a \texttt{https://[t√∫nel].trycloudflare.com/webhook/[workflow-id]}, donde n8n maneja la ruta interna para actualizaciones. Al activar el workflow, n8n registra el webhook con la API de Telegram, confirmando que recibir√° notificaciones push para cada mensaje enviado al bot. Esto asegura que el trigger se active solo para nuestro bot, filtrando interacciones irrelevantes por defecto.
	
	\subsection{Paso 3: Estructura del Mensaje Entrante y Preparaci√≥n para el Flujo}
	
	Cuando un usuario env√≠a un mensaje al bot, como \texttt{/analysis sp500}, el trigger de Telegram recibe un JSON estructurado con la actualizaci√≥n completa del mensaje, que incluye metadatos del usuario y el chat para contextualizar la interacci√≥n. La estructura t√≠pica es un array de objetos con campos como \texttt{update\_id} (identificador √∫nico de la actualizaci√≥n), \texttt{message} (detalles del mensaje), que contiene \texttt{message\_id}, \texttt{from} (informaci√≥n del emisor con \texttt{id} num√©rico), \texttt{chat} (detalles del chat con el mismo \texttt{id}), \texttt{date} (timestamp), \texttt{text} (contenido del mensaje, e.g., ``/analysis sp500'') y \texttt{entities} (anotaciones como comandos de bot). Por ejemplo, un mensaje de prueba genera algo como:

	\newpage
	
	\begin{lstlisting}[style=consola, language=bash]
[
  {
    "update_id": 177484711,
    "message": {
      "message_id": 20,
      "from": {
        "id": 1065676350,
        "is_bot": false,
        "first_name": "Jordi",
        "last_name": "Blasco Lozano",
        "username": "jbloz",
        "language_code": "es"
      },
      "chat": {
        "id": 1065676350,
        "first_name": "Jordi",
        "last_name": "Blasco Lozano",
        "username": "jbloz",
        "type": "private"
      },
      "date": 1761410615,
      "text": "/analysis sp500",
      "entities": [
        {
          "offset": 0,
          "length": 9,
          "type": "bot_command"
        }
      ]
    }
  }
]
	\end{lstlisting}
	
	Este formato detallado es √∫til para tracking, pero para nuestro flujo de an√°lisis de sentimiento, necesitamos simplificarlo extrayendo solo los elementos esenciales: el texto despu√©s del comando (e.g., ``sp500'') y el ID del usuario (e.g., 1065676350) para respuestas posteriores. Por lo tanto, transformamos el JSON entrante en una estructura minimalista como:
	
	\begin{lstlisting}[style=consola, language=bash]
[
  {
    "text": "sp500",
    "id": 1065676350
  }
]
	\end{lstlisting}
	
	Esta reformateaci√≥n se logra en los nodos subsiguientes (filtro y c√≥digo JavaScript), eliminando ruido como nombres, timestamps y entidades, para pasar datos limpios al resto del workflow sin sobrecargar el procesamiento. De esta manera, el trigger inicializa el flujo de forma eficiente, preparando el terreno para la validaci√≥n y extracci√≥n de comandos espec√≠ficos como \texttt{/analysis}. Podr√≠amos haber puesto unicamente que devolviera el text y cuando quisi√©ramos mandar el mensaje de vuelta sacar el id del trigger, pero decidimos que era mejor estrategia estructurar solamente los datos necesarios para manejarlos a posteriori desde estos nodos unicamente.
	
	En la pr√≥xima secci√≥n, detallaremos c√≥mo filtramos mensajes irrelevantes y refinamos el texto extra√≠do mediante nodos de c√≥digo.
	
	\section{Transformaci√≥n y filtrado de los datos de entrada: Nodos Filter y JavaScript en n8n}
	
	Una vez que el trigger de Telegram recibe los mensajes enviados al bot, el siguiente paso en el workflow de n8n es limpiar y transformar los datos para quedarnos solo con la informaci√≥n relevante para nuestro an√°lisis burs√°til. El objetivo aqu√≠ es filtrar solo los mensajes que contengan el comando \texttt{/analysis} y extraer el texto posterior al comando (el ticker o nombre del activo) junto al identificador de usuario para personalizar las respuestas en el resto del flujo.
	
	\subsection{Paso 1: Nodo de Filter}
	
	El nodo Filter funciona como una puerta de acceso, permitiendo √∫nicamente los mensajes que cumplen una condici√≥n clave: que el mensaje incluya la palabra \texttt{/analysis}. As√≠, si un usuario escribe cualquier otro texto o comando, el flujo lo descartar√° autom√°ticamente y no avanzar√° a las siguientes etapas. Esta l√≥gica reduce ruido y asegura que solo procesamos peticiones v√°lidas y estructuradas para el an√°lisis del sentimiento de mercado.
	
	\textbf{Configuraci√≥n:}
	\begin{itemize}
		\item Campo a filtrar: \texttt{message.text} (dentro del JSON recibido).
		\item Condici√≥n: contiene \texttt{/analysis}.
		\item Acci√≥n: si pasa el filtro, el mensaje sigue el flujo; si no, se descarta.
	\end{itemize}
	
	\subsection{Paso 2: Nodo Code (JavaScript)}
	
	El nodo \textbf{Code} o \textbf{Code in JavaScript} es donde transformamos el mensaje filtrado en un formato sencillo y funcional para el resto del workflow. El bloque de c√≥digo que has incluido cumple tres funciones principales:
	
	\begin{itemize}
		\item Toma el texto completo del mensaje (\texttt{fullText}) y el identificador del usuario (\texttt{ID}).
		\item Localiza el comando \texttt{/analysis} y extrae √∫nicamente el texto que viene detr√°s (lo que el usuario quiere analizar: un ticker, √≠ndice, etc.).
		\item Devuelve un objeto JSON minimalista con el formato exactamente necesario para la API siguiente: el \texttt{text} limpio y el \texttt{id} para enviar la respuesta al usuario correcto.
	\end{itemize}
	
	\begin{lstlisting}[style=consola, language=bash]
const fullText = $input.item.json.message.text;
const ID = $input.item.json.message.from.id;
const command = "/analysis ";

// Extrae todo lo que viene despu√©s de /analysis
const textAfterCommand = fullText.includes(command) 
? fullText.substring(fullText.indexOf(command) + command.length).trim()
: fullText;

return {
json: {
    text: textAfterCommand,
    id: ID
}
};\end{lstlisting}
	
	Gracias a esto, transformamos un mensaje de entrada complejo, como el que hemos visto en el paso anterior en una salida simple y limpia como esta:
	
\begin{lstlisting}[style=consola, language=bash]
[
  {
    "text": "sp500",
    "id": 1065676350
  }
]\end{lstlisting}
	
	Esto prepara el mensaje para ser enviado a la API de busqueda de noticias o cualquier otro m√≥dulo de procesamiento posterior, facilitando al m√°ximo la integraci√≥n y manteniendo limpio el flujo de trabajo en n8n.
	
	\section{HTTP Request para buscar noticias con Perplexity API}
	
	Una vez que hemos procesado el mensaje del usuario para obtener √∫nicamente el texto relevante (el ticker o nombre del activo financiero), el siguiente paso del workflow consiste en consultar la API de Perplexity para recopilar las noticias m√°s recientes sobre ese activo. Esta b√∫squeda es fundamental para que el LLM realice el an√°lisis de sentimiento con informaci√≥n actualizada y relevante.
	
	\subsection{Perplexity, subscripci√≥n y API}
	
	Perplexity ofrece varias maneras de acceder a su API Pro, incluyendo un periodo de prueba de 12 meses si asocias una cuenta de PayPal, as√≠ como promociones puntuales para ciertas cuentas o estudiantes. En nuestro caso, disponemos de una suscripci√≥n Pro, que nos da acceso prioritario tanto a modelos avanzados como a la propia API. Esta suscripci√≥n incluye 5 ‚Ç¨ de cr√©dito API cada mes, suficiente para mantener proyectos ligeros y pruebas recurrentes.
	
	\subsection{Acceso a la API de b√∫squeda}
	
	Con la suscripci√≥n Pro, tenemos acceso directo a la API, incluida una de las funciones m√°s potentes: la \textbf{API de b√∫squeda}. La documentaci√≥n oficial de Perplexity proporciona un ejemplo de integraci√≥n r√°pida mediante cURL (que podemos copiar y pegar al m√≥dulo de HTTP Request en n8n), autenticando cada llamada con nuestra clave personal. All√≠ mismo se detallan par√°metros de uso como el l√≠mite de resultados o el m√°ximo de tokens por noticia.
	
	\subsection{Ejemplo pr√°ctico de la integraci√≥n en n8n}
	
	Creamos un m√≥dulo HTTP Request justo despu√©s del bloque de JavaScript, usando estos ajustes:
	\begin{itemize}
		\item \textbf{URL:} \texttt{https://api.perplexity.ai/search}
		\item \textbf{M√©todo:} POST
		\item \textbf{Autenticaci√≥n:} Bearer Token (clave API de Perplexity)
		\item \textbf{Cuerpo (JSON):}
	\end{itemize}
	
	\begin{lstlisting}[style=consola, language=bash]
{
  "query": [
      "Last news about {{ $json.text }}",
      "{{ $json.text }} financial asset forecasts",
      "{{ $json.text }} financial asset last results"
  ],
  "max_results": 8
}\end{lstlisting}
	
	Aqu√≠, \texttt{\{\{ \$json.text \}\}} toma el ticker del paso anterior para personalizar la b√∫squeda. Para enriquecer los resultados, no nos limitamos a una sola consulta, sino que realizamos tres b√∫squedas simult√°neas sobre el mismo activo, solicitando noticias recientes, previsiones y los √∫ltimos resultados financieros. Esto proporciona un contexto mucho m√°s rico para el an√°lisis posterior.
	
	El input a este m√≥dulo ser√° el JSON minimalista del paso previo (por ejemplo, \texttt{\{"text": "sp500", "id": 1065676350\}}), asegurando as√≠ que la consulta est√© siempre alineada con el comando que ha solicitado el usuario.
	
	Con esto, cada vez que el bot lo requiera, buscar√° siempre las 8 noticias m√°s relevantes para cada una de las tres consultas, proporcionando a nuestro agente LLM los datos actualizados y preparados para la toma de decisiones de inversi√≥n y el an√°lisis de sentimiento.
	
	\section{Formateo de Noticias y Preparaci√≥n para el An√°lisis}
	
	Tras la llamada exitosa a la API de Perplexity (\texttt{HTTP Request}), el workflow recibe un objeto JSON complejo que contiene una lista de noticias. Esta estructura, aunque rica en datos (con URLs, snippets, metadatos, etc.), no es un formato √≥ptimo para ser analizada directamente por un Modelo de Lenguaje (LLM). Para maximizar la precisi√≥n del an√°lisis y minimizar el consumo de tokens, es crucial ``limpiar'' y ``condensar'' esta informaci√≥n.
	
	Esta tarea la realiza el nodo \textbf{Code in JavaScript1}. Su funci√≥n es iterar sobre el array de resultados de la b√∫squeda y concatenar la informaci√≥n esencial de cada noticia (como el t√≠tulo y el resumen o snippet) en un √∫nico bloque de texto plano. Este bloque de texto se formatea de manera legible, separando cada noticia para que el LLM pueda distinguirlas.
	
	Adem√°s, este nodo inicializa un contador de reintentos (\texttt{retry\_count: 0}), que ser√° clave para la l√≥gica de correcci√≥n que veremos m√°s adelante.
	
	El resultado es un objeto JSON que contiene el \texttt{text} (nombre del activo), las \texttt{noticias} (el texto plano con toda la informaci√≥n) y el \texttt{retry\_count}, sirviendo como la √∫nica ``fuente de verdad'' para los siguientes pasos de an√°lisis.
	
	\section{Arquitectura de ``Actor-Cr√≠tico'': El Proceso de An√°lisis y Revisi√≥n}
	
	Para asegurar una alta fiabilidad en la respuesta y evitar las ``alucinaciones'' (respuestas incorrectas o inventadas) de la IA, implementamos una arquitectura avanzada de dos agentes conocida como \textbf{Actor-Cr√≠tico}. En lugar de confiar en un solo agente, uno genera el an√°lisis (el ``Actor'') y un segundo agente, m√°s estricto, lo revisa (el ``Cr√≠tico'').
	
	\subsection{Paso 1: El Actor - AI Agent (Analista)}
	
	El primer agente, \textbf{AI Agent}, act√∫a como el ``Analista'' y es el coraz√≥n del sistema de an√°lisis de sentimiento. Este nodo utiliza un modelo de lenguaje avanzado (espec√≠ficamente \texttt{alibaba/tongyi-deepresearch-30b-a3b:free} a trav√©s de OpenRouter) configurado con un \textit{system prompt} especializado y un \textit{user prompt} din√°mico que inyecta el contexto de noticias.
	
	\subsubsection{Configuraci√≥n del System Prompt}
	
	El \textit{system prompt} define la personalidad y las restricciones del modelo:
	
	\begin{lstlisting}[style=consola, language=bash]
Eres un analista financiero experto que basa sus analisis en la 
informacion que te proporcionan, responderas siempre en Espanol de 
Espana. En texto plano, como se indique en el prompt, sin usar en 
ningun caso markdown.
\end{lstlisting}
	
	Esta configuraci√≥n es cr√≠tica porque:
	\begin{itemize}
		\item Establece el \textbf{rol experto} del modelo como analista financiero.
		\item Enfatiza que debe basarse \textbf{√∫nicamente en la informaci√≥n proporcionada}, reduciendo alucinaciones.
		\item Especifica el idioma (Espa√±ol de Espa√±a) para consistencia ling√º√≠stica.
		\item Proh√≠be expl√≠citamente el uso de markdown, forzando texto plano para facilitar el parsing posterior.
	\end{itemize}
	
	\subsubsection{Estructura del User Prompt}
	
	El \textit{user prompt} es din√°mico y se construye inyectando el contexto de noticias del nodo anterior:
	
	\begin{lstlisting}[style=consola, language=bash]
Contexto de noticias sobre {{ $('Code in JavaScript').first().json.text }}:
{{ $('Code in JavaScript1').first().json.noticias }}

---
TAREA OBLIGATORIA:
Analiza el contexto de noticias anterior sobre 
{{ $('Code in JavaScript').first().json.text }}.
Responde UNICAMENTE con el siguiente formato exacto, sin saludos 
ni explicaciones adicionales:

Compra: [X]%
Mantener: [Y]%
Vender: [Z]%
Expectativa: [Escribe 'Alcista' o 'Bajista']
Resumen: [Escribe aqui un resumen de un parrafo de las noticias 
que justifican el analisis]
\end{lstlisting}
	
	La estructura del prompt incluye:
	\begin{enumerate}
		\item \textbf{Contexto de noticias:} Todo el texto concatenado de las 8 noticias obtenidas de Perplexity, con t√≠tulos, res√∫menes y enlaces.
		\item \textbf{Nombre del activo:} Se repite dos veces el item de b√∫squeda para reforzar el contexto (e.g., ``sp500'', ``AAPL'').
		\item \textbf{Instrucciones estrictas:} Se enfatiza ``√öNICAMENTE'' y ``formato exacto'' para minimizar desviaciones.
		\item \textbf{Formato de salida esperado:} Se especifica l√≠nea por l√≠nea el formato requerido.
	\end{enumerate}
	
	\subsubsection{Salida del Agente}
	
	El modelo responde con un formato de texto estricto:
	
	\begin{lstlisting}[style=consola, language=bash]
Compra: [X]%
Mantener: [Y]%
Vender: [Z]%
Expectativa: [Alcista o Bajista]
Resumen: [Un resumen de un parrafo de las noticias...]
\end{lstlisting}
	
	Este formato simple basado en texto es f√°cil de generar para el modelo, pero a√∫n puede contener errores (porcentajes incorrectos, res√∫menes inventados, formato inconsistente). Por eso, la salida de este nodo (un √∫nico \textit{string} de texto) pasa inmediatamente al Cr√≠tico para validaci√≥n antes de ser enviada al usuario.
	
	\subsection{Paso 2: El Cr√≠tico - AI Agent1 (Revisor)}
	
	Aqu√≠ es donde entra el control de calidad. El segundo agente, \textbf{AI Agent1}, act√∫a como el ``Revisor''. Este nodo recibe dos entradas cruciales:
	\begin{enumerate}
		\item El \textbf{contexto de noticias original} (del nodo \texttt{Code in JavaScript1}).
		\item El \textbf{an√°lisis en texto} generado por el primer agente (la salida de \texttt{AI Agent}).
	\end{enumerate}
	
	La tarea del Revisor no es generar un an√°lisis, sino \textbf{juzgar} el an√°lisis del Actor. Su \textit{prompt} le instruye a comparar el resumen y los porcentajes con las noticias originales y determinar si el an√°lisis est√° 100\% justificado por ellas.
	
	Para que su decisi√≥n sea utilizable por el workflow, forzamos su salida a un simple ``True'' o ``False''.
	
	\section{Flujo Condicional: El Bucle de Correcci√≥n y Fusi√≥n}
	
	La salida del Revisor nos permite crear un flujo condicional para simular un ``bucle de re-intento''.
	
	\subsection{Paso 1: El Nodo If}
	
	Este nodo lee la salida del \texttt{AI Agent1} (Revisor). Su condici√≥n es si la salida contiene ``True'' y si el an√°lisis del \texttt{AI Agent} contiene las palabras clave esperadas para poder realizar el reformateo posterior(``Compra:'', ``Mantener:'', ``Resumen:'' etc.). Esto crea dos ramas:
	\begin{itemize}
		\item \textbf{Rama ``True'' (√âxito):} El an√°lisis es bueno y puede continuar.
		\item \textbf{Rama ``False'' (Error):} El an√°lisis es malo o incompleto y necesita ser rehecho.
	\end{itemize}
	
	\subsection{Paso 2: La Rama ``False'' (Re-intento del Actor)}
	
	Esta rama es la clave de nuestra l√≥gica de ``loop''. Cuando el an√°lisis es \texttt{false}, el flujo se dirige al nodo \texttt{Code Increment Retry}, que aumenta en uno el contador de reintentos. Despu√©s, el nodo \texttt{If Max Retry} comprueba si se ha alcanzado el l√≠mite de x reintentos (ahora mismo no lo tenemos limitado porque siempre salen bien los an√°lisis). Si no se ha alcanzado, el flujo vuelve al \texttt{AI Agent} para que genere un nuevo an√°lisis. Si se alcanza el l√≠mite, el flujo termina en un error.
	
	\subsection{Paso 3: La Fusi√≥n de Ramas}
	
	La rama ``True'' del nodo \texttt{If} se conecta directamente al nodo final de formateo, \texttt{Code in JavaScript2}.
	
	\section{Formateo Final y Respuesta al Usuario}
	
	\subsection{Code in JavaScript2 (Formateador Avanzado)}
	
	Este es el nodo de c√≥digo que prepara la respuesta final. Su trabajo es tomar el an√°lisis \textit{final y aprobado} y convertirlo en el mensaje que ver√° el usuario. Su l√≥gica interna hace lo siguiente:
	
	\begin{enumerate}
		\item \textbf{Extracci√≥n con Regex:} Toma el \texttt{aiResponse} (el texto del an√°lisis correcto del \texttt{AI Agent}) y utiliza \textbf{Expresiones Regulares (\texttt{.match()})} para extraer los valores de cada l√≠nea (ej: \texttt{Compra: (\textbackslash d+)\%}).
		\item \textbf{Formateo HTML:} Finalmente, construye el mensaje de respuesta usando \textbf{etiquetas HTML} (como \texttt{<b>} para negrita) para darle estilo, incluyendo emojis (\texttt{üìà} o \texttt{üìâ}) seg√∫n la expectativa.
	\end{enumerate}
	
	\subsection{Send a text message (Env√≠o por Telegram)}
	
	Este es el √∫ltimo nodo. Recibe el \texttt{responseText} (el texto HTML) del nodo anterior y lo env√≠a al usuario. La configuraci√≥n clave aqu√≠ es:
	\begin{itemize}
		\item \textbf{Chat ID:} Se obtiene din√°micamente del tercer nodo del flujo (\texttt{JavaScript.id}), asegurando que la respuesta siempre vuelva al usuario que hizo la petici√≥n.
		\item \textbf{Parse Mode:} Se establece en \textbf{HTML} para que Telegram interprete correctamente las etiquetas \texttt{<b>} y \texttt{‚Ä¢} que hemos definido.
	\end{itemize}
	
	Este ciclo completo asegura que el usuario reciba una respuesta no solo r√°pida, sino tambi√©n verificada, fiable y formateada profesionalmente.


\section{Pruebas}

Poniendo a prueba la aplicaci√≥n obtuvimos los siguientes resultados:

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image1.PNG}
    \caption{An√°lisis de oro}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image2.PNG}
    \caption{An√°lisis de LSMC.DE}
\end{subfigure}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image3.PNG}
    \caption{An√°lisis de sp500}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image4.PNG}
    \caption{An√°lisis de NUKL.DE}
\end{subfigure}

\end{figure}

\end{document}